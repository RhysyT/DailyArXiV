<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Interest — 23rd October 2025</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<style>
  :root {
    --ink: #1b1f23; --muted: #666; --rule: #eee; --accent: #004aad;
  }
  body { font: 16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;
         margin: 2rem auto; max-width: 900px; padding: 0 1rem; color: var(--ink); }
  h1 { margin: 0 0 .25rem 0; font-size: 1.8rem; }
  .sub { color: var(--muted); margin: 0 0 1.25rem 0; }
  h2 { margin: 1.5rem 0 .5rem; font-size: 1.15rem; color: var(--muted); font-weight: 600; }
  article { padding: .75rem 0; border-top: 1px solid var(--rule); }
  a.title { text-decoration: none; font-weight: 600; color: inherit; }
  a.title:hover { text-decoration: underline; color: var(--accent); }
  .id { color:#999; font-size:.9rem; margin-left:.5rem; }
  details { margin-top: .35rem; }
  summary { cursor: pointer; color: var(--accent); outline: none; }
  .num { font-variant-numeric: tabular-nums; width: 2.25rem; display:inline-block; color:#999; }
</style>

    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        packages: {'[+]': ['textmacros']}
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
    </script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</head>
<body>
<h1>Interest</h1>
<div class="sub">23rd October 2025 · Physics Education · 2 entries</div>
<h2>Physics Education</h2><article><div><span class="num"> 1.</span> <a class="title" href="https://arxiv.org/abs/2510.19288" target="_blank" rel="noopener noreferrer">Mapping the AI Divide in Undergraduate Education: Community Detection in Disciplinary Networks and Survey Evidence</a><span class="id">[2510.19288]</span></div><details><summary>Abstract</summary><div><p>As artificial intelligence-generated content (AIGC) reshapes knowledge acquisition, higher education faces growing inequities that demand systematic mapping and intervention. We map the AI divide in undergraduate education by combining network science with survey evidence from 301 students at Nanjing University, one of China's leading institutions in AI education. Drawing on course enrolment patterns to construct a disciplinary network, we identify four distinct student communities: science dominant, science peripheral, social sciences &amp; science, and humanities and social sciences. Survey results reveal significant disparities in AIGC literacy and motivational efficacy, with science dominant students outperforming humanities and social sciences peers. Ordinary least squares (OLS) regression shows that motivational efficacy–particularly skill efficacy–partially mediates this gap, whereas usage efficacy does not mediate at the evaluation level, indicating a dissociation between perceived utility and critical engagement. Our findings demonstrate that curriculum structure and cross-disciplinary integration are key determinants of technological fluency. This work provides a scalable framework for diagnosing and addressing the AI divide through institutional design.</p></div></details></article><article><div><span class="num"> 2.</span> <a class="title" href="https://arxiv.org/abs/2505.17950" target="_blank" rel="noopener noreferrer">Evaluating NLP Embedding Models for Handling Science-Specific Symbolic Expressions in Student Texts</a><span class="id">[2505.17950]</span></div><details><summary>Abstract</summary><div><p>In recent years, natural language processing (NLP) has become integral to educational data mining, particularly in the analysis of student-generated language products. For research and assessment purposes, so-called embedding models are typically employed to generate numeric representations of text that capture its semantic content for use in subsequent quantitative analyses. Yet when it comes to science-related language, symbolic expressions such as equations and formulas introduce challenges that current embedding models struggle to address. Existing research studies and practical applications often either overlook these challenges or remove symbolic expressions altogether, potentially leading to biased research findings and diminished performance of practical applications. This study therefore explores how contemporary embedding models differ in their capability to process and interpret science-related symbolic expressions. To this end, various embedding models are evaluated using physics-specific symbolic expressions drawn from authentic student responses, with performance assessed via two approaches: 1) similarity-based analyses and 2) integration into a machine learning pipeline. Our findings reveal significant differences in model performance, with OpenAI's GPT-text-embedding-3-large outperforming all other examined models, though its advantage over other models was moderate rather than decisive. Overall, this study underscores the importance for educational data mining researchers and practitioners of carefully selecting NLP embedding models when working with science-related language products that include symbolic expressions. The code and (partial) data are available at this https URL .</p></div></details></article></body>
</html>